{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6285345,"sourceType":"datasetVersion","datasetId":3614295},{"sourceId":6306409,"sourceType":"datasetVersion","datasetId":3628120}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-16T09:28:32.503155Z","iopub.execute_input":"2023-08-16T09:28:32.503518Z","iopub.status.idle":"2023-08-16T09:28:32.524421Z","shell.execute_reply.started":"2023-08-16T09:28:32.503486Z","shell.execute_reply":"2023-08-16T09:28:32.522649Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/cohenintextorg2/Chohen intext org.xlsx\n/kaggle/input/d1intextorg1/Arshad D1.xlsx\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:28:49.576969Z","iopub.execute_input":"2023-08-16T09:28:49.578057Z","iopub.status.idle":"2023-08-16T09:28:49.584902Z","shell.execute_reply.started":"2023-08-16T09:28:49.578009Z","shell.execute_reply":"2023-08-16T09:28:49.583440Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the data\ndata =  pd.read_excel('/kaggle/input/cohenintextorg2/Chohen intext org.xlsx')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:28:52.058955Z","iopub.execute_input":"2023-08-16T09:28:52.060003Z","iopub.status.idle":"2023-08-16T09:28:54.100374Z","shell.execute_reply.started":"2023-08-16T09:28:52.059954Z","shell.execute_reply":"2023-08-16T09:28:54.099105Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                              CitingPaperId  \\\n0  1872080baa7d30ec8fb87be9a65358cd3a7fb649   \n1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b   \n2  d9f3207db0c79a3b154f3875c9760cc6b056904b   \n3  88b86556857f4374842d2af2e359576806239175   \n4  df2f5d253798a83b31b1df8d4a343bdcdfeb492b   \n\n                               CitedPaperId  \\\n0  894be9b4ea46a5c422e81ef3c241072d4c73fdc0   \n1  b6642e19efb8db5623b3cc4eef1c5822a6151107   \n2  2cc6ff899bf17666ad35893524a4d61624555ed7   \n3  a5bb0ff1a026944d2a47a155462959af2b8505a8   \n4  d91f4ce0487619e1ff3f30facd959e2530bde365   \n\n                                              String  Label  \n0  However, how frataxin interacts with the Fe-S ...      0  \n1  In the study by Hickey et al. (2012), spikes w...      0  \n2  By clustering with lowly aggressive close kin ...      0  \n3  Ophthalmic symptoms are rare manifestations of...      0  \n4  Recent studies identified Wee1 as a potential ...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CitingPaperId</th>\n      <th>CitedPaperId</th>\n      <th>String</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649</td>\n      <td>894be9b4ea46a5c422e81ef3c241072d4c73fdc0</td>\n      <td>However, how frataxin interacts with the Fe-S ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b</td>\n      <td>b6642e19efb8db5623b3cc4eef1c5822a6151107</td>\n      <td>In the study by Hickey et al. (2012), spikes w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b</td>\n      <td>2cc6ff899bf17666ad35893524a4d61624555ed7</td>\n      <td>By clustering with lowly aggressive close kin ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>88b86556857f4374842d2af2e359576806239175</td>\n      <td>a5bb0ff1a026944d2a47a155462959af2b8505a8</td>\n      <td>Ophthalmic symptoms are rare manifestations of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>df2f5d253798a83b31b1df8d4a343bdcdfeb492b</td>\n      <td>d91f4ce0487619e1ff3f30facd959e2530bde365</td>\n      <td>Recent studies identified Wee1 as a potential ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data = data.drop(['CitingPaperId', 'CitedPaperId'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:28:57.526723Z","iopub.execute_input":"2023-08-16T09:28:57.527436Z","iopub.status.idle":"2023-08-16T09:28:57.544012Z","shell.execute_reply.started":"2023-08-16T09:28:57.527399Z","shell.execute_reply":"2023-08-16T09:28:57.542905Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:28:59.752359Z","iopub.execute_input":"2023-08-16T09:28:59.752787Z","iopub.status.idle":"2023-08-16T09:28:59.764870Z","shell.execute_reply.started":"2023-08-16T09:28:59.752754Z","shell.execute_reply":"2023-08-16T09:28:59.763442Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                              String  Label\n0  However, how frataxin interacts with the Fe-S ...      0\n1  In the study by Hickey et al. (2012), spikes w...      0\n2  By clustering with lowly aggressive close kin ...      0\n3  Ophthalmic symptoms are rare manifestations of...      0\n4  Recent studies identified Wee1 as a potential ...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>String</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>However, how frataxin interacts with the Fe-S ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In the study by Hickey et al. (2012), spikes w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>By clustering with lowly aggressive close kin ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ophthalmic symptoms are rare manifestations of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Recent studies identified Wee1 as a potential ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Spliting the data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['String'], data['Label'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:29:02.049031Z","iopub.execute_input":"2023-08-16T09:29:02.049405Z","iopub.status.idle":"2023-08-16T09:29:02.059224Z","shell.execute_reply.started":"2023-08-16T09:29:02.049375Z","shell.execute_reply":"2023-08-16T09:29:02.057712Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#  Preprocess the text data","metadata":{}},{"cell_type":"code","source":"# Preprocess the text data\nstop_words = set(stopwords.words('english'))\ndef preprocess(text):\n    text = text.lower()\n    text = ''.join([word for word in text if word not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\nX_train = X_train.apply(preprocess)\nX_test = X_test.apply(preprocess)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:29:04.996056Z","iopub.execute_input":"2023-08-16T09:29:04.996447Z","iopub.status.idle":"2023-08-16T09:29:08.908933Z","shell.execute_reply.started":"2023-08-16T09:29:04.996415Z","shell.execute_reply":"2023-08-16T09:29:08.907630Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Tokenize the text data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:29:11.787153Z","iopub.execute_input":"2023-08-16T09:29:11.787526Z","iopub.status.idle":"2023-08-16T09:29:12.423694Z","shell.execute_reply.started":"2023-08-16T09:29:11.787496Z","shell.execute_reply":"2023-08-16T09:29:12.422395Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Pad the sequences to a fixed length\nmax_length = 300\nX_train = pad_sequences(X_train, maxlen=max_length, padding='post')\nX_test = pad_sequences(X_test, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:32.596883Z","iopub.execute_input":"2023-08-16T13:18:32.597265Z","iopub.status.idle":"2023-08-16T13:18:32.634722Z","shell.execute_reply.started":"2023-08-16T13:18:32.597235Z","shell.execute_reply":"2023-08-16T13:18:32.633358Z"},"trusted":true},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":"# Taning the word2vec  model and siting window size","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Assuming X_train is a NumPy array of sentences\n# Convert the array elements to strings if they are not already\nsentences = [str(sentence) for sentence in X_train]\n\n# Split each sentence into a list of words\nsplit_sentences = [sentence.split() for sentence in sentences]\n\n# Train the Word2Vec model\nw2v_model = Word2Vec(split_sentences, vector_size=300, window=20, min_count=5, workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:18.921795Z","iopub.execute_input":"2023-08-16T13:18:18.922176Z","iopub.status.idle":"2023-08-16T13:18:29.762720Z","shell.execute_reply.started":"2023-08-16T13:18:18.922147Z","shell.execute_reply":"2023-08-16T13:18:29.761803Z"},"trusted":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":"# Create a weight matrix for the embedding layer\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:29.764767Z","iopub.execute_input":"2023-08-16T13:18:29.765151Z","iopub.status.idle":"2023-08-16T13:18:29.828076Z","shell.execute_reply.started":"2023-08-16T13:18:29.765120Z","shell.execute_reply":"2023-08-16T13:18:29.826854Z"},"trusted":true},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":"# Balanceing dataset","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:35.976646Z","iopub.execute_input":"2023-08-16T13:18:35.977785Z","iopub.status.idle":"2023-08-16T13:18:36.254705Z","shell.execute_reply.started":"2023-08-16T13:18:35.977738Z","shell.execute_reply":"2023-08-16T13:18:36.253629Z"},"trusted":true},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":"# # Check the balance of the dataset after applying SMOTE","metadata":{}},{"cell_type":"code","source":"# Check the balance of the dataset after applying SMOTE\nunique, counts = np.unique(y_train_smote, return_counts=True)\nclass_counts = dict(zip(unique, counts))\nprint(\"Class Counts after SMOTE:\", class_counts)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:38.740127Z","iopub.execute_input":"2023-08-16T13:18:38.741268Z","iopub.status.idle":"2023-08-16T13:18:38.749317Z","shell.execute_reply.started":"2023-08-16T13:18:38.741221Z","shell.execute_reply":"2023-08-16T13:18:38.747820Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Class Counts after SMOTE: {0: 3613, 1: 3613}\n","output_type":"stream"}],"execution_count":105},{"cell_type":"markdown","source":"# Define the deep learning models","metadata":{}},{"cell_type":"code","source":"# Define the CNN model\ncnn_model = Sequential()\ncnn_model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\ncnn_model.add(Conv1D(128, 5, activation='relu'))\ncnn_model.add(MaxPooling1D(5))\ncnn_model.add(Conv1D(128, 5, activation='relu'))\ncnn_model.add(MaxPooling1D(5))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dense(1, activation='sigmoid'))\n\ncnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n#Define GRU Model\n\nfrom keras.layers import GRU\n\ngru_model = Sequential()\ngru_model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\ngru_model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\ngru_model.add(Dense(1, activation='sigmoid'))\n\ngru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Define LSTM Model\n\nfrom keras.layers import LSTM\n\n# Define the LSTM model\nlstm_model = Sequential()\nlstm_model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\nlstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:40.976291Z","iopub.execute_input":"2023-08-16T13:18:40.976969Z","iopub.status.idle":"2023-08-16T13:18:41.897508Z","shell.execute_reply.started":"2023-08-16T13:18:40.976933Z","shell.execute_reply":"2023-08-16T13:18:41.896017Z"},"trusted":true},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":"# Runing the model**","metadata":{}},{"cell_type":"code","source":"cnn_model.fit(X_train_smote,y_train_smote, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n# Train the GRU model\ngru_model.fit(X_train_smote, y_train_smote, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n# Train the LSTM model\nlstm_model.fit(X_train_smote, y_train_smote, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:18:48.029885Z","iopub.execute_input":"2023-08-16T13:18:48.031101Z","iopub.status.idle":"2023-08-16T14:09:29.569615Z","shell.execute_reply.started":"2023-08-16T13:18:48.031062Z","shell.execute_reply":"2023-08-16T14:09:29.568361Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\n226/226 [==============================] - 26s 111ms/step - loss: 0.6931 - accuracy: 0.5339 - val_loss: 0.6781 - val_accuracy: 0.5726\nEpoch 2/10\n226/226 [==============================] - 25s 112ms/step - loss: 0.6773 - accuracy: 0.5801 - val_loss: 0.6895 - val_accuracy: 0.5283\nEpoch 3/10\n226/226 [==============================] - 25s 112ms/step - loss: 0.6694 - accuracy: 0.5918 - val_loss: 0.7167 - val_accuracy: 0.5296\nEpoch 4/10\n226/226 [==============================] - 26s 113ms/step - loss: 0.6636 - accuracy: 0.6061 - val_loss: 0.7172 - val_accuracy: 0.5424\nEpoch 5/10\n226/226 [==============================] - 25s 112ms/step - loss: 0.6604 - accuracy: 0.6138 - val_loss: 0.7066 - val_accuracy: 0.5630\nEpoch 6/10\n226/226 [==============================] - 25s 111ms/step - loss: 0.6502 - accuracy: 0.6223 - val_loss: 0.7310 - val_accuracy: 0.5328\nEpoch 7/10\n226/226 [==============================] - 25s 113ms/step - loss: 0.6451 - accuracy: 0.6234 - val_loss: 0.6946 - val_accuracy: 0.5623\nEpoch 8/10\n226/226 [==============================] - 26s 114ms/step - loss: 0.6341 - accuracy: 0.6353 - val_loss: 0.7517 - val_accuracy: 0.5476\nEpoch 9/10\n226/226 [==============================] - 26s 114ms/step - loss: 0.6279 - accuracy: 0.6385 - val_loss: 0.7417 - val_accuracy: 0.5353\nEpoch 10/10\n226/226 [==============================] - 27s 119ms/step - loss: 0.6223 - accuracy: 0.6456 - val_loss: 0.7355 - val_accuracy: 0.5572\nEpoch 1/10\n226/226 [==============================] - 148s 642ms/step - loss: 0.6936 - accuracy: 0.4953 - val_loss: 0.6943 - val_accuracy: 0.4280\nEpoch 2/10\n226/226 [==============================] - 144s 636ms/step - loss: 0.6936 - accuracy: 0.4913 - val_loss: 0.6946 - val_accuracy: 0.4280\nEpoch 3/10\n226/226 [==============================] - 142s 631ms/step - loss: 0.6933 - accuracy: 0.5021 - val_loss: 0.6941 - val_accuracy: 0.4280\nEpoch 4/10\n226/226 [==============================] - 143s 631ms/step - loss: 0.6935 - accuracy: 0.4979 - val_loss: 0.6908 - val_accuracy: 0.5720\nEpoch 5/10\n226/226 [==============================] - 142s 628ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6951 - val_accuracy: 0.4280\nEpoch 6/10\n226/226 [==============================] - 144s 635ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6940 - val_accuracy: 0.4280\nEpoch 7/10\n226/226 [==============================] - 143s 631ms/step - loss: 0.6932 - accuracy: 0.5058 - val_loss: 0.6922 - val_accuracy: 0.5720\nEpoch 8/10\n226/226 [==============================] - 143s 632ms/step - loss: 0.6934 - accuracy: 0.4932 - val_loss: 0.6961 - val_accuracy: 0.4280\nEpoch 9/10\n226/226 [==============================] - 142s 629ms/step - loss: 0.6933 - accuracy: 0.4979 - val_loss: 0.6926 - val_accuracy: 0.5720\nEpoch 10/10\n226/226 [==============================] - 143s 632ms/step - loss: 0.6934 - accuracy: 0.4881 - val_loss: 0.6887 - val_accuracy: 0.5720\nEpoch 1/10\n226/226 [==============================] - 140s 605ms/step - loss: 0.6934 - accuracy: 0.4932 - val_loss: 0.6938 - val_accuracy: 0.4280\nEpoch 2/10\n226/226 [==============================] - 133s 589ms/step - loss: 0.6933 - accuracy: 0.5024 - val_loss: 0.6922 - val_accuracy: 0.5720\nEpoch 3/10\n226/226 [==============================] - 134s 591ms/step - loss: 0.6933 - accuracy: 0.4921 - val_loss: 0.6925 - val_accuracy: 0.5720\nEpoch 4/10\n226/226 [==============================] - 133s 590ms/step - loss: 0.6933 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4280\nEpoch 5/10\n226/226 [==============================] - 134s 595ms/step - loss: 0.6933 - accuracy: 0.4989 - val_loss: 0.6927 - val_accuracy: 0.5720\nEpoch 6/10\n226/226 [==============================] - 132s 585ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6923 - val_accuracy: 0.5720\nEpoch 7/10\n226/226 [==============================] - 131s 580ms/step - loss: 0.6933 - accuracy: 0.4896 - val_loss: 0.6936 - val_accuracy: 0.4280\nEpoch 8/10\n226/226 [==============================] - 131s 579ms/step - loss: 0.6933 - accuracy: 0.4965 - val_loss: 0.6936 - val_accuracy: 0.4280\nEpoch 9/10\n226/226 [==============================] - 132s 584ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6936 - val_accuracy: 0.4280\nEpoch 10/10\n226/226 [==============================] - 132s 582ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6924 - val_accuracy: 0.5720\n","output_type":"stream"},{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f6102fe20b0>"},"metadata":{}}],"execution_count":107},{"cell_type":"markdown","source":"Cheecking Performance on different window sizes","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding, GRU, LSTM\nfrom imblearn.over_sampling import SMOTE\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom gensim.models import Word2Vec\n\n# Load the data\ndata = pd.read_excel('/kaggle/input/cohenintextorg2/Chohen intext org.xlsx')\nX_train, X_test, y_train, y_test = train_test_split(data['String'], data['Label'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:13:12.058902Z","iopub.execute_input":"2024-10-06T05:13:12.059943Z","iopub.status.idle":"2024-10-06T05:13:12.987587Z","shell.execute_reply.started":"2024-10-06T05:13:12.059903Z","shell.execute_reply":"2024-10-06T05:13:12.986387Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Preprocess the text data\nstop_words = set(stopwords.words('english'))\ndef preprocess(text):\n    text = text.lower()\n    text = ''.join([word for word in text if word not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\nX_train = X_train.apply(preprocess)\nX_test = X_test.apply(preprocess)\n\n# Tokenize and pad the sequences\nmax_length = 300\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_train = pad_sequences(X_train, maxlen=max_length, padding='post')\nX_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n\nvocab_size = len(tokenizer.word_index) + 1\n\n# Apply SMOTE to balance the dataset\nsmote = SMOTE()\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:13:16.725304Z","iopub.execute_input":"2024-10-06T05:13:16.725685Z","iopub.status.idle":"2024-10-06T05:13:19.607798Z","shell.execute_reply.started":"2024-10-06T05:13:16.725655Z","shell.execute_reply":"2024-10-06T05:13:19.606578Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Evaluate different window sizes\nwindow_sizes = [2, 3, 5, 8, 10, 12]\nresults = {'Window': [], 'Model': [], 'Accuracy': [], 'Precision': []}\n\nfor window_size in window_sizes:\n    print(f\"Training Word2Vec with window size: {window_size}\")\n    \n    # Convert the array elements to strings\n    sentences = [str(sentence) for sentence in X_train]\n    split_sentences = [sentence.split() for sentence in sentences]\n    \n    # Train the Word2Vec model\n    w2v_model = Word2Vec(split_sentences, vector_size=300, window=window_size, min_count=5, workers=4)\n    \n    # Create embedding matrix\n    embedding_matrix = np.zeros((vocab_size, 300))\n    for word, i in tokenizer.word_index.items():\n        if word in w2v_model.wv:\n            embedding_matrix[i] = w2v_model.wv[word]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:13:50.996564Z","iopub.execute_input":"2024-10-06T05:13:50.996928Z","iopub.status.idle":"2024-10-06T05:14:31.651515Z","shell.execute_reply.started":"2024-10-06T05:13:50.996899Z","shell.execute_reply":"2024-10-06T05:14:31.650428Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training Word2Vec with window size: 2\nTraining Word2Vec with window size: 3\nTraining Word2Vec with window size: 5\nTraining Word2Vec with window size: 8\nTraining Word2Vec with window size: 10\nTraining Word2Vec with window size: 12\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Define models\ndef define_cnn():\n        model = Sequential()\n        model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n        model.add(Conv1D(128, 5, activation='relu'))\n        model.add(MaxPooling1D(5))\n        model.add(Conv1D(128, 5, activation='relu'))\n        model.add(MaxPooling1D(5))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return model\n\ndef define_gru():\n        model = Sequential()\n        model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return model\n\ndef define_lstm():\n        model = Sequential()\n        model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        return model","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:16:31.583054Z","iopub.execute_input":"2024-10-06T05:16:31.583461Z","iopub.status.idle":"2024-10-06T05:16:31.595042Z","shell.execute_reply.started":"2024-10-06T05:16:31.583427Z","shell.execute_reply":"2024-10-06T05:16:31.593634Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Train and evaluate CNN\ncnn_model = define_cnn()\ncnn_model.fit(X_train_smote, y_train_smote, epochs=5, batch_size=32, verbose=0)\ny_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")\ncnn_acc = accuracy_score(y_test, y_pred)\ncnn_prec = precision_score(y_test, y_pred)\nresults['Window'].append(window_size)\nresults['Model'].append('CNN')\nresults['Accuracy'].append(cnn_acc)\nresults['Precision'].append(cnn_prec)\n    \n    # Train and evaluate GRU\ngru_model = define_gru()\ngru_model.fit(X_train_smote, y_train_smote, epochs=5, batch_size=32, verbose=0)\ny_pred = (gru_model.predict(X_test) > 0.5).astype(\"int32\")\ngru_acc = accuracy_score(y_test, y_pred)\ngru_prec = precision_score(y_test, y_pred)\nresults['Window'].append(window_size)\nresults['Model'].append('GRU')\nresults['Accuracy'].append(gru_acc)\nresults['Precision'].append(gru_prec)\n    \n    # Train and evaluate LSTM\nlstm_model = define_lstm()\nlstm_model.fit(X_train_smote, y_train_smote, epochs=5, batch_size=32, verbose=0)\ny_pred = (lstm_model.predict(X_test) > 0.5).astype(\"int32\")\nlstm_acc = accuracy_score(y_test, y_pred)\nlstm_prec = precision_score(y_test, y_pred)\nresults['Window'].append(window_size)\nresults['Model'].append('LSTM')\nresults['Accuracy'].append(lstm_acc)\nresults['Precision'].append(lstm_prec)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T06:24:53.814569Z","iopub.execute_input":"2024-10-06T06:24:53.814908Z","iopub.status.idle":"2024-10-06T06:46:44.151750Z","shell.execute_reply.started":"2024-10-06T06:24:53.814876Z","shell.execute_reply":"2024-10-06T06:46:44.150620Z"},"trusted":true},"outputs":[{"name":"stdout","text":"49/49 [==============================] - 2s 36ms/step\n49/49 [==============================] - 3s 64ms/step\n49/49 [==============================] - 4s 81ms/step\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Convert results to DataFrame for easy charting\nresults_df = pd.DataFrame(results)\n\n# Plotting results\nplt.figure(figsize=(12, 6))\nfor model in ['CNN', 'GRU', 'LSTM']:\n    model_data = results_df[results_df['Model'] == model]\n    plt.plot(model_data['Window'], model_data['Accuracy'], label=f'{model} Accuracy')\n    plt.plot(model_data['Window'], model_data['Precision'], label=f'{model} Precision', linestyle='--')\n\nplt.xlabel('Window Size')\nplt.ylabel('Score')\nplt.title('Accuracy and Precision by Window Size for Different Models')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"outputs":[],"execution_count":null}]}